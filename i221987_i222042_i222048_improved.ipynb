{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Improved Network Intrusion Detection using Deep Learning\n",
        "\n",
        "This notebook contains improved implementations of CNN, LSTM, and CNN-LSTM models for network intrusion detection on NSL-KDD dataset.\n",
        "\n",
        "## Improvements:\n",
        "1. **Bug Fixes**: Fixed typo in column alignment\n",
        "2. **Enhanced Architectures**: Added BatchNormalization, improved regularization\n",
        "3. **Class Imbalance Handling**: Implemented class weights for balanced training\n",
        "4. **Advanced Training**: Early stopping, learning rate scheduling, model checkpointing\n",
        "5. **Better Evaluation**: ROC-AUC curves, classification reports, enhanced visualizations\n",
        "6. **Code Quality**: Better error handling and modular design\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 1 — Setup: installs, imports, seed, GPU check, constants\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, callbacks\n",
        "from tensorflow.keras.regularizers import l2\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score, \n",
        "    confusion_matrix, matthews_corrcoef, roc_auc_score, \n",
        "    roc_curve, classification_report\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Reproducibility\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "tf.random.set_seed(SEED)\n",
        "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
        "\n",
        "# Aggressively disable XLA JIT compilation to avoid shape compatibility issues\n",
        "tf.config.optimizer.set_jit(False)\n",
        "os.environ['TF_XLA_FLAGS'] = '--tf_xla_enable_xla_devices=false'\n",
        "os.environ['TF_DISABLE_XLA'] = '1'\n",
        "# Disable XLA for all operations\n",
        "tf.config.experimental.enable_op_determinism()\n",
        "\n",
        "print(\"TF version:\", tf.__version__)\n",
        "print(\"GPUs:\", tf.config.list_physical_devices('GPU'))\n",
        "\n",
        "# Mixed precision disabled to avoid shape compatibility issues\n",
        "# If needed, can be enabled but requires careful handling of dtypes\n",
        "# try:\n",
        "#     policy = tf.keras.mixed_precision.Policy('mixed_float16')\n",
        "#     tf.keras.mixed_precision.set_global_policy(policy)\n",
        "#     print(\"Mixed precision enabled\")\n",
        "# except:\n",
        "#     print(\"Mixed precision not available\")\n",
        "print(\"Using float32 precision (mixed precision disabled for stability)\")\n",
        "print(\"XLA JIT compilation disabled to avoid shape compatibility issues\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Constants\n",
        "RESULTS_DIR = \"./results\"\n",
        "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
        "\n",
        "# Training parameters\n",
        "EPOCHS = 50\n",
        "BATCH_SIZE = 128\n",
        "VALIDATION_SPLIT = 0.2\n",
        "PATIENCE = 10  # For early stopping\n",
        "\n",
        "sns.set(style=\"whitegrid\", palette=\"muted\")\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "print(\"Setup complete.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 2 — NSL-KDD loading and preprocessing\n",
        "# Using direct file paths - update these to match your dataset location\n",
        "train_fp = \"/kaggle/input/nslkdd/KDDTrain+.txt\"\n",
        "test_fp = \"/kaggle/input/nslkdd/KDDTest+.txt\"\n",
        "\n",
        "# Alternative paths for local execution\n",
        "if not os.path.exists(train_fp):\n",
        "    train_fp = \"./KDDTrain+.txt\"\n",
        "    test_fp = \"./KDDTest+.txt\"\n",
        "\n",
        "print(\"Using NSL-KDD files:\")\n",
        "print(f\"  Train: {train_fp}\")\n",
        "print(f\"  Test: {test_fp}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read the data (NSL-KDD has no headers)\n",
        "try:\n",
        "    df_train = pd.read_csv(train_fp, header=None)\n",
        "    df_test = pd.read_csv(test_fp, header=None)\n",
        "    print(\"Data loaded successfully!\")\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"Error: {e}\")\n",
        "    print(\"Please ensure the dataset files are in the correct location.\")\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add column names based on NSL-KDD documentation\n",
        "columns = [\n",
        "    'duration', 'protocol_type', 'service', 'flag', 'src_bytes', 'dst_bytes', \n",
        "    'land', 'wrong_fragment', 'urgent', 'hot', 'num_failed_logins', 'logged_in', \n",
        "    'num_compromised', 'root_shell', 'su_attempted', 'num_root', 'num_file_creations', \n",
        "    'num_shells', 'num_access_files', 'num_outbound_cmds', 'is_host_login', \n",
        "    'is_guest_login', 'count', 'srv_count', 'serror_rate', 'srv_serror_rate', \n",
        "    'rerror_rate', 'srv_rerror_rate', 'same_srv_rate', 'diff_srv_rate', \n",
        "    'srv_diff_host_rate', 'dst_host_count', 'dst_host_srv_count', \n",
        "    'dst_host_same_srv_rate', 'dst_host_diff_srv_rate', 'dst_host_same_src_port_rate', \n",
        "    'dst_host_srv_diff_host_rate', 'dst_host_serror_rate', 'dst_host_srv_serror_rate', \n",
        "    'dst_host_rerror_rate', 'dst_host_srv_rerror_rate', 'attack_type', 'difficulty'\n",
        "]\n",
        "\n",
        "df_train.columns = columns\n",
        "df_test.columns = columns\n",
        "\n",
        "print(\"Train shape:\", df_train.shape)\n",
        "print(\"Test shape:\", df_test.shape)\n",
        "print(\"\\nFirst few rows:\")\n",
        "print(df_train.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert to binary labels (normal = 0, attack = 1)\n",
        "df_train['label'] = df_train['attack_type'].apply(lambda x: 0 if x == 'normal' else 1)\n",
        "df_test['label'] = df_test['attack_type'].apply(lambda x: 0 if x == 'normal' else 1)\n",
        "\n",
        "print(\"Label distribution - Train:\")\n",
        "train_dist = df_train['label'].value_counts()\n",
        "print(train_dist)\n",
        "print(f\"\\nTrain class ratio (Normal:Attack): {train_dist[0]/train_dist[1]:.2f}\")\n",
        "\n",
        "print(\"\\nLabel distribution - Test:\")\n",
        "test_dist = df_test['label'].value_counts()\n",
        "print(test_dist)\n",
        "print(f\"\\nTest class ratio (Normal:Attack): {test_dist[0]/test_dist[1]:.2f}\")\n",
        "\n",
        "# Visualize class distribution\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
        "train_dist.plot(kind='bar', ax=axes[0], title='Train Set Class Distribution', color=['skyblue', 'coral'])\n",
        "axes[0].set_xlabel('Class (0=Normal, 1=Attack)')\n",
        "axes[0].set_ylabel('Count')\n",
        "test_dist.plot(kind='bar', ax=axes[1], title='Test Set Class Distribution', color=['skyblue', 'coral'])\n",
        "axes[1].set_xlabel('Class (0=Normal, 1=Attack)')\n",
        "axes[1].set_ylabel('Count')\n",
        "plt.tight_layout()\n",
        "plt.savefig(f'{RESULTS_DIR}/class_distribution.png', dpi=150)\n",
        "plt.show()\n",
        "\n",
        "# Drop unnecessary columns\n",
        "df_train = df_train.drop(['attack_type', 'difficulty'], axis=1)\n",
        "df_test = df_test.drop(['attack_type', 'difficulty'], axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Handle categorical columns (protocol_type, service, flag)\n",
        "categorical_columns = ['protocol_type', 'service', 'flag']\n",
        "\n",
        "# One-hot encode categorical variables\n",
        "df_train = pd.get_dummies(df_train, columns=categorical_columns, prefix=categorical_columns)\n",
        "df_test = pd.get_dummies(df_test, columns=categorical_columns, prefix=categorical_columns)\n",
        "\n",
        "# Align columns (some services might be missing in test set)\n",
        "# FIXED BUG: Changed df_test.colum to df_test.columns\n",
        "train_cols = df_train.columns\n",
        "test_cols = df_test.columns\n",
        "\n",
        "# Add missing columns to test set\n",
        "missing_cols = set(train_cols) - set(test_cols)\n",
        "if missing_cols:\n",
        "    print(f\"Adding {len(missing_cols)} missing columns to test set\")\n",
        "    for col in missing_cols:\n",
        "        if col != 'label':\n",
        "            df_test[col] = 0\n",
        "\n",
        "# Remove extra columns from test set\n",
        "extra_cols = set(test_cols) - set(train_cols)\n",
        "if extra_cols:\n",
        "    print(f\"Removing {len(extra_cols)} extra columns from test set\")\n",
        "    df_test = df_test.drop(columns=list(extra_cols))\n",
        "\n",
        "# Reorder test columns to match train\n",
        "df_test = df_test[train_cols]\n",
        "\n",
        "print(f\"\\nAfter preprocessing:\")\n",
        "print(f\"  Train shape: {df_train.shape}\")\n",
        "print(f\"  Test shape: {df_test.shape}\")\n",
        "print(f\"  Number of features: {len(train_cols) - 1}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare features and labels\n",
        "feature_cols = [col for col in df_train.columns if col != 'label']\n",
        "\n",
        "# Scale numerical features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(df_train[feature_cols].astype(float))\n",
        "X_test = scaler.transform(df_test[feature_cols].astype(float))\n",
        "\n",
        "# Calculate class weights for imbalanced dataset\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "class_weights = compute_class_weight(\n",
        "    'balanced',\n",
        "    classes=np.unique(df_train['label']),\n",
        "    y=df_train['label']\n",
        ")\n",
        "class_weight_dict = {i: class_weights[i] for i in range(len(class_weights))}\n",
        "print(f\"Class weights: {class_weight_dict}\")\n",
        "\n",
        "# Reshape for CNN/LSTM (samples, timesteps, features)\n",
        "timesteps = X_train.shape[1]\n",
        "X_train = X_train.reshape((-1, timesteps, 1))\n",
        "X_test = X_test.reshape((-1, timesteps, 1))\n",
        "\n",
        "y_train = df_train['label'].values\n",
        "y_test = df_test['label'].values\n",
        "\n",
        "print(f\"\\nFinal shapes:\")\n",
        "print(f\"  X_train: {X_train.shape}, X_test: {X_test.shape}\")\n",
        "print(f\"  y_train: {y_train.shape}, y_test: {y_test.shape}\")\n",
        "print(f\"  Timesteps: {timesteps}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 3 — Improved Model definitions\n",
        "# Based on original architectures with safe enhancements\n",
        "\n",
        "def build_cnn_improved(input_shape):\n",
        "    \"\"\"Enhanced CNN model - based on original with regularization\"\"\"\n",
        "    model = keras.Sequential([\n",
        "        layers.Conv1D(64, 3, activation='relu', input_shape=input_shape),\n",
        "        layers.MaxPooling1D(2),\n",
        "        layers.Conv1D(128, 3, activation='relu'),\n",
        "        layers.GlobalAveragePooling1D(),\n",
        "        layers.Dense(64, activation='relu', kernel_regularizer=l2(0.001)),\n",
        "        layers.Dropout(0.3),\n",
        "        layers.Dense(2, activation='softmax')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "def build_lstm_improved(input_shape):\n",
        "    \"\"\"Enhanced LSTM model - based on original with bidirectional\"\"\"\n",
        "    model = keras.Sequential([\n",
        "        layers.Bidirectional(layers.LSTM(64), input_shape=input_shape),\n",
        "        layers.Dropout(0.3),\n",
        "        layers.Dense(32, activation='relu', kernel_regularizer=l2(0.001)),\n",
        "        layers.Dense(2, activation='softmax')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "def build_cnn_lstm_improved(input_shape):\n",
        "    \"\"\"Enhanced CNN-LSTM hybrid - based on original architecture\"\"\"\n",
        "    model = keras.Sequential([\n",
        "        layers.Conv1D(64, 3, activation='relu', input_shape=input_shape),\n",
        "        layers.MaxPooling1D(2),\n",
        "        layers.LSTM(64),\n",
        "        layers.Dropout(0.3),\n",
        "        layers.Dense(32, activation='relu', kernel_regularizer=l2(0.001)),\n",
        "        layers.Dense(2, activation='softmax')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "print(\"Model architectures defined successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 4 — Enhanced Training and Evaluation Functions\n",
        "\n",
        "def create_callbacks(model_name):\n",
        "    \"\"\"Create callbacks for training\"\"\"\n",
        "    callback_list = [\n",
        "        callbacks.EarlyStopping(\n",
        "            monitor='val_loss',\n",
        "            patience=PATIENCE,\n",
        "            restore_best_weights=True,\n",
        "            verbose=1\n",
        "        ),\n",
        "        callbacks.ReduceLROnPlateau(\n",
        "            monitor='val_loss',\n",
        "            factor=0.5,\n",
        "            patience=5,\n",
        "            min_lr=1e-7,\n",
        "            verbose=1\n",
        "        ),\n",
        "        callbacks.ModelCheckpoint(\n",
        "            filepath=f'{RESULTS_DIR}/{model_name}_best.h5',\n",
        "            monitor='val_loss',\n",
        "            save_best_only=True,\n",
        "            verbose=1\n",
        "        ),\n",
        "        callbacks.CSVLogger(f'{RESULTS_DIR}/{model_name}_training.log')\n",
        "    ]\n",
        "    return callback_list\n",
        "\n",
        "def train_model_improved(model, name, class_weight=None):\n",
        "    \"\"\"Enhanced training function with callbacks and class weights\"\"\"\n",
        "    # Disable XLA JIT compilation to avoid shape compatibility issues\n",
        "    tf.config.optimizer.set_jit(False)\n",
        "    \n",
        "    # Use Adam optimizer with learning rate scheduling\n",
        "    initial_lr = 0.001\n",
        "    optimizer = keras.optimizers.Adam(learning_rate=initial_lr)\n",
        "    \n",
        "    # Use only accuracy metric to avoid shape compatibility issues with precision/recall\n",
        "    # Run eagerly to bypass graph compilation issues\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy'],\n",
        "        run_eagerly=False  # Set to True if still having issues\n",
        "    )\n",
        "    \n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Training {name}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"Model summary:\")\n",
        "    model.summary()\n",
        "    \n",
        "    callback_list = create_callbacks(name)\n",
        "    \n",
        "    # Test model with a small batch first to catch shape errors early\n",
        "    try:\n",
        "        # Test forward pass\n",
        "        test_input = X_train[:1]\n",
        "        _ = model(test_input, training=False)\n",
        "        print(\"Model forward pass test successful!\")\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR: Model forward pass failed: {e}\")\n",
        "        print(\"This indicates a shape mismatch in the model architecture.\")\n",
        "        raise\n",
        "    \n",
        "    history = model.fit(\n",
        "        X_train, y_train,\n",
        "        validation_split=VALIDATION_SPLIT,\n",
        "        epochs=EPOCHS,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        class_weight=class_weight,\n",
        "        callbacks=callback_list,\n",
        "        verbose=1\n",
        "    )\n",
        "    \n",
        "    return model, history\n",
        "\n",
        "def evaluate_model_improved(model, name, history=None):\n",
        "    \"\"\"Enhanced evaluation with ROC curves and detailed metrics\"\"\"\n",
        "    y_pred_proba = model.predict(X_test, verbose=0, batch_size=BATCH_SIZE)\n",
        "    y_pred = np.argmax(y_pred_proba, axis=1)\n",
        "    y_pred_proba_class1 = y_pred_proba[:, 1]  # Probability of attack class\n",
        "    \n",
        "    # Calculate metrics\n",
        "    metrics = {\n",
        "        'accuracy': accuracy_score(y_test, y_pred),\n",
        "        'precision': precision_score(y_test, y_pred, zero_division=0),\n",
        "        'recall': recall_score(y_test, y_pred, zero_division=0),\n",
        "        'f1': f1_score(y_test, y_pred, zero_division=0),\n",
        "        'mcc': matthews_corrcoef(y_test, y_pred),\n",
        "        'roc_auc': roc_auc_score(y_test, y_pred_proba_class1)\n",
        "    }\n",
        "    \n",
        "    # Confusion matrix\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    \n",
        "    # Create comprehensive visualization\n",
        "    fig = plt.figure(figsize=(16, 5))\n",
        "    \n",
        "    # Plot 1: Confusion Matrix\n",
        "    ax1 = plt.subplot(1, 3, 1)\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
        "                xticklabels=['Normal', 'Attack'], \n",
        "                yticklabels=['Normal', 'Attack'],\n",
        "                ax=ax1)\n",
        "    ax1.set_title(f'Confusion Matrix - {name}')\n",
        "    ax1.set_ylabel('True Label')\n",
        "    ax1.set_xlabel('Predicted Label')\n",
        "    \n",
        "    # Plot 2: ROC Curve\n",
        "    ax2 = plt.subplot(1, 3, 2)\n",
        "    fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba_class1)\n",
        "    ax2.plot(fpr, tpr, linewidth=2, label=f'ROC (AUC = {metrics[\"roc_auc\"]:.4f})')\n",
        "    ax2.plot([0, 1], [0, 1], 'k--', linewidth=1)\n",
        "    ax2.set_xlabel('False Positive Rate')\n",
        "    ax2.set_ylabel('True Positive Rate')\n",
        "    ax2.set_title(f'ROC Curve - {name}')\n",
        "    ax2.legend()\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Plot 3: Training History (if available)\n",
        "    if history is not None:\n",
        "        ax3 = plt.subplot(1, 3, 3)\n",
        "        ax3.plot(history.history['loss'], label='Train Loss', linewidth=2)\n",
        "        ax3.plot(history.history['val_loss'], label='Val Loss', linewidth=2)\n",
        "        ax3.set_xlabel('Epoch')\n",
        "        ax3.set_ylabel('Loss')\n",
        "        ax3.set_title(f'Training History - {name}')\n",
        "        ax3.legend()\n",
        "        ax3.grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'{RESULTS_DIR}/{name}_evaluation.png', dpi=150, bbox_inches='tight')\n",
        "    plt.close()\n",
        "    \n",
        "    # Print detailed results\n",
        "    print(f\"\\n{name} Results:\")\n",
        "    print(\"-\" * 60)\n",
        "    for metric, value in metrics.items():\n",
        "        print(f\"  {metric.upper():15s}: {value:.4f}\")\n",
        "    \n",
        "    # Classification report\n",
        "    print(f\"\\nClassification Report:\")\n",
        "    print(classification_report(y_test, y_pred, \n",
        "                                target_names=['Normal', 'Attack'],\n",
        "                                digits=4))\n",
        "    \n",
        "    return metrics, history\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 5 — Run experiments with improved models\n",
        "models = {\n",
        "    'CNN_Improved': build_cnn_improved((timesteps, 1)),\n",
        "    'LSTM_Improved': build_lstm_improved((timesteps, 1)),\n",
        "    'CNN_LSTM_Improved': build_cnn_lstm_improved((timesteps, 1))\n",
        "}\n",
        "\n",
        "print(\"Starting model training with improvements...\")\n",
        "print(f\"Training parameters:\")\n",
        "print(f\"  Epochs: {EPOCHS}\")\n",
        "print(f\"  Batch size: {BATCH_SIZE}\")\n",
        "print(f\"  Validation split: {VALIDATION_SPLIT}\")\n",
        "print(f\"  Early stopping patience: {PATIENCE}\")\n",
        "print(f\"  Class weights: {class_weight_dict}\")\n",
        "\n",
        "results = []\n",
        "histories = {}\n",
        "\n",
        "for name, model in models.items():\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Processing {name}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    trained_model, history = train_model_improved(model, name, class_weight=class_weight_dict)\n",
        "    metrics, _ = evaluate_model_improved(trained_model, name, history)\n",
        "    metrics['model'] = name\n",
        "    results.append(metrics)\n",
        "    histories[name] = history\n",
        "    \n",
        "    print(f\"\\n{name} training completed!\")\n",
        "    print(f\"{'='*60}\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 6 — Comprehensive Results Analysis and Visualization\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"FINAL RESULTS SUMMARY\")\n",
        "print(f\"{'='*60}\\n\")\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "print(results_df.to_string(index=False))\n",
        "\n",
        "# Save results\n",
        "results_df.to_csv(f'{RESULTS_DIR}/nsl_kdd_results_improved.csv', index=False)\n",
        "print(f\"\\nResults saved to: {RESULTS_DIR}/nsl_kdd_results_improved.csv\")\n",
        "\n",
        "# Create comprehensive comparison plots\n",
        "fig = plt.figure(figsize=(18, 12))\n",
        "\n",
        "# Plot 1: Metrics Comparison Bar Chart\n",
        "ax1 = plt.subplot(2, 3, 1)\n",
        "metrics_to_plot = ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']\n",
        "x = np.arange(len(results_df))\n",
        "width = 0.15\n",
        "\n",
        "for i, metric in enumerate(metrics_to_plot):\n",
        "    offset = (i - len(metrics_to_plot)/2) * width + width/2\n",
        "    ax1.bar(x + offset, results_df[metric], width, label=metric.replace('_', ' ').title())\n",
        "\n",
        "ax1.set_xlabel('Models')\n",
        "ax1.set_ylabel('Score')\n",
        "ax1.set_title('Model Performance Comparison')\n",
        "ax1.set_xticks(x)\n",
        "ax1.set_xticklabels(results_df['model'], rotation=45, ha='right')\n",
        "ax1.set_ylim(0, 1.05)\n",
        "ax1.legend(loc='lower right')\n",
        "ax1.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# Plot 2: MCC Comparison\n",
        "ax2 = plt.subplot(2, 3, 2)\n",
        "bars = ax2.bar(results_df['model'], results_df['mcc'], color=['#3498db', '#e74c3c', '#2ecc71'])\n",
        "ax2.set_ylabel('MCC Score')\n",
        "ax2.set_title('Matthews Correlation Coefficient')\n",
        "ax2.set_ylim(0, 1)\n",
        "ax2.grid(True, alpha=0.3, axis='y')\n",
        "for i, (bar, val) in enumerate(zip(bars, results_df['mcc'])):\n",
        "    ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
        "             f'{val:.4f}', ha='center', va='bottom')\n",
        "\n",
        "# Plot 3: ROC-AUC Comparison\n",
        "ax3 = plt.subplot(2, 3, 3)\n",
        "bars = ax3.bar(results_df['model'], results_df['roc_auc'], color=['#9b59b6', '#f39c12', '#1abc9c'])\n",
        "ax3.set_ylabel('ROC-AUC Score')\n",
        "ax3.set_title('ROC-AUC Comparison')\n",
        "ax3.set_ylim(0, 1)\n",
        "ax3.grid(True, alpha=0.3, axis='y')\n",
        "for i, (bar, val) in enumerate(zip(bars, results_df['roc_auc'])):\n",
        "    ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
        "             f'{val:.4f}', ha='center', va='bottom')\n",
        "\n",
        "# Plot 4-6: Training History Comparison\n",
        "for idx, (name, history) in enumerate(histories.items(), start=4):\n",
        "    ax = plt.subplot(2, 3, idx)\n",
        "    ax.plot(history.history['loss'], label='Train Loss', linewidth=2)\n",
        "    ax.plot(history.history['val_loss'], label='Val Loss', linewidth=2)\n",
        "    ax.plot(history.history['accuracy'], label='Train Acc', linewidth=2, linestyle='--')\n",
        "    ax.plot(history.history['val_accuracy'], label='Val Acc', linewidth=2, linestyle='--')\n",
        "    ax.set_xlabel('Epoch')\n",
        "    ax.set_ylabel('Score')\n",
        "    ax.set_title(f'{name} Training History')\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(f'{RESULTS_DIR}/comprehensive_comparison.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Find best model\n",
        "best_model_idx = results_df['f1'].idxmax()\n",
        "best_model = results_df.loc[best_model_idx, 'model']\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"BEST MODEL: {best_model}\")\n",
        "print(f\"{'='*60}\")\n",
        "print(results_df.loc[best_model_idx].to_string())\n",
        "print(f\"{'='*60}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Additional Analysis: Feature Importance and Model Comparison Table\n",
        "\n",
        "print(\"\\nDetailed Model Comparison:\")\n",
        "print(\"=\"*80)\n",
        "comparison_table = results_df.set_index('model').T\n",
        "print(comparison_table.to_string())\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Save comparison table\n",
        "comparison_table.to_csv(f'{RESULTS_DIR}/detailed_comparison.csv')\n",
        "\n",
        "# Create a summary report\n",
        "with open(f'{RESULTS_DIR}/summary_report.txt', 'w') as f:\n",
        "    f.write(\"Network Intrusion Detection - Improved Model Results\\n\")\n",
        "    f.write(\"=\"*80 + \"\\n\\n\")\n",
        "    f.write(f\"Dataset: NSL-KDD\\n\")\n",
        "    f.write(f\"Training samples: {len(y_train)}\\n\")\n",
        "    f.write(f\"Test samples: {len(y_test)}\\n\")\n",
        "    f.write(f\"Features: {timesteps}\\n\\n\")\n",
        "    f.write(\"Model Performance:\\n\")\n",
        "    f.write(\"-\"*80 + \"\\n\")\n",
        "    f.write(results_df.to_string(index=False))\n",
        "    f.write(\"\\n\\n\")\n",
        "    f.write(f\"Best Model (by F1-Score): {best_model}\\n\")\n",
        "    f.write(\"=\"*80 + \"\\n\")\n",
        "\n",
        "print(f\"\\nSummary report saved to: {RESULTS_DIR}/summary_report.txt\")\n",
        "print(\"\\nAll results and visualizations have been saved successfully!\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
