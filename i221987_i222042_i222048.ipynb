{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJCo9NNsXrBI"
      },
      "source": [
        "New Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 1 — Setup: installs, imports, seed, GPU check, constants\n",
        "import os, random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, matthews_corrcoef\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Reproducibility\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "tf.random.set_seed(SEED)\n",
        "\n",
        "print(\"TF version:\", tf.__version__)\n",
        "print(\"GPUs:\", tf.config.list_physical_devices('GPU'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Constants\n",
        "RESULTS_DIR = \"/content/results\"\n",
        "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
        "\n",
        "sns.set(style=\"whitegrid\")\n",
        "print(\"Setup complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 2 — NSL-KDD loading and preprocessing\n",
        "# Using direct file paths since files are already in Kaggle input\n",
        "train_fp = \"/kaggle/input/nslkdd/KDDTrain+.txt\"\n",
        "test_fp = \"/kaggle/input/nslkdd/KDDTest+.txt\"\n",
        "\n",
        "print(\"Using NSL-KDD files:\", train_fp, test_fp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read the data (NSL-KDD has no headers)\n",
        "df_train = pd.read_csv(train_fp, header=None)\n",
        "df_test = pd.read_csv(test_fp, header=None)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add column names based on NSL-KDD documentation\n",
        "columns = [\n",
        "    'duration', 'protocol_type', 'service', 'flag', 'src_bytes', 'dst_bytes', \n",
        "    'land', 'wrong_fragment', 'urgent', 'hot', 'num_failed_logins', 'logged_in', \n",
        "    'num_compromised', 'root_shell', 'su_attempted', 'num_root', 'num_file_creations', \n",
        "    'num_shells', 'num_access_files', 'num_outbound_cmds', 'is_host_login', \n",
        "    'is_guest_login', 'count', 'srv_count', 'serror_rate', 'srv_serror_rate', \n",
        "    'rerror_rate', 'srv_rerror_rate', 'same_srv_rate', 'diff_srv_rate', \n",
        "    'srv_diff_host_rate', 'dst_host_count', 'dst_host_srv_count', \n",
        "    'dst_host_same_srv_rate', 'dst_host_diff_srv_rate', 'dst_host_same_src_port_rate', \n",
        "    'dst_host_srv_diff_host_rate', 'dst_host_serror_rate', 'dst_host_srv_serror_rate', \n",
        "    'dst_host_rerror_rate', 'dst_host_srv_rerror_rate', 'attack_type', 'difficulty'\n",
        "]\n",
        "\n",
        "df_train.columns = columns\n",
        "df_test.columns = columns\n",
        "\n",
        "print(\"Train shape:\", df_train.shape)\n",
        "print(\"Test shape:\", df_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert to binary labels (normal = 0, attack = 1)\n",
        "df_train['label'] = df_train['attack_type'].apply(lambda x: 0 if x == 'normal' else 1)\n",
        "df_test['label'] = df_test['attack_type'].apply(lambda x: 0 if x == 'normal' else 1)\n",
        "\n",
        "print(\"Label distribution - Train:\")\n",
        "print(df_train['label'].value_counts())\n",
        "print(\"\\nLabel distribution - Test:\")\n",
        "print(df_test['label'].value_counts())\n",
        "\n",
        "# Drop unnecessary columns\n",
        "df_train = df_train.drop(['attack_type', 'difficulty'], axis=1)\n",
        "df_test = df_test.drop(['attack_type', 'difficulty'], axis=1)\n",
        "\n",
        "# Handle categorical columns (protocol_type, service, flag)\n",
        "categorical_columns = ['protocol_type', 'service', 'flag']\n",
        "\n",
        "# One-hot encode categorical variables to avoid unknown category issues\n",
        "df_train = pd.get_dummies(df_train, columns=categorical_columns, prefix=categorical_columns)\n",
        "df_test = pd.get_dummies(df_test, columns=categorical_columns, prefix=categorical_columns)\n",
        "\n",
        "# Align columns (some services might be missing in test set)\n",
        "train_cols = df_train.columns\n",
        "test_cols = df_test.columns\n",
        "\n",
        "# Add missing columns to test set\n",
        "for col in train_cols:\n",
        "    if col not in test_cols and col != 'label':\n",
        "        df_test[col] = 0\n",
        "\n",
        "# Reorder test columns to match train\n",
        "df_test = df_test[train_cols]\n",
        "\n",
        "print(f\"After preprocessing - Train shape: {df_train.shape}, Test shape: {df_test.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare features and labels\n",
        "feature_cols = [col for col in df_train.columns if col != 'label']\n",
        "\n",
        "# Scale numerical features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(df_train[feature_cols].astype(float))\n",
        "X_test = scaler.transform(df_test[feature_cols].astype(float))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Reshape for CNN/LSTM (samples, timesteps, features)\n",
        "timesteps = X_train.shape[1]\n",
        "X_train = X_train.reshape((-1, timesteps, 1))\n",
        "X_test = X_test.reshape((-1, timesteps, 1))\n",
        "\n",
        "y_train = df_train['label'].values\n",
        "y_test = df_test['label'].values\n",
        "\n",
        "print(f\"Final shapes - X_train: {X_train.shape}, X_test: {X_test.shape}\")\n",
        "print(f\"y_train: {y_train.shape}, y_test: {y_test.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Cell 3 — Model definitions\n",
        "def build_cnn(input_shape):\n",
        "    model = keras.Sequential([\n",
        "        layers.Conv1D(64, 3, activation='relu', input_shape=input_shape),\n",
        "        layers.MaxPooling1D(2),\n",
        "        layers.Conv1D(128, 3, activation='relu'),\n",
        "        layers.GlobalAveragePooling1D(),\n",
        "        layers.Dense(64, activation='relu'),\n",
        "        layers.Dropout(0.3),\n",
        "        layers.Dense(2, activation='softmax')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "def build_lstm(input_shape):\n",
        "    model = keras.Sequential([\n",
        "        layers.LSTM(64, input_shape=input_shape),\n",
        "        layers.Dropout(0.3),\n",
        "        layers.Dense(32, activation='relu'),\n",
        "        layers.Dense(2, activation='softmax')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "def build_cnn_lstm(input_shape):\n",
        "    model = keras.Sequential([\n",
        "        layers.Conv1D(64, 3, activation='relu', input_shape=input_shape),\n",
        "        layers.MaxPooling1D(2),\n",
        "        layers.LSTM(64),\n",
        "        layers.Dropout(0.3),\n",
        "        layers.Dense(32, activation='relu'),\n",
        "        layers.Dense(2, activation='softmax')\n",
        "    ])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Cell 4 — Training and evaluation\n",
        "def train_model(model, name):\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    \n",
        "    print(f\"Training {name}...\")\n",
        "    history = model.fit(X_train, y_train,\n",
        "                       validation_split=0.2,\n",
        "                       epochs=10,  # Reduced for faster execution\n",
        "                       batch_size=128,\n",
        "                       verbose=1)\n",
        "    \n",
        "    return model, history\n",
        "\n",
        "def evaluate_model(model, name):\n",
        "    y_pred_proba = model.predict(X_test, verbose=0)\n",
        "    y_pred = np.argmax(y_pred_proba, axis=1)\n",
        "    \n",
        "    metrics = {\n",
        "        'accuracy': accuracy_score(y_test, y_pred),\n",
        "        'precision': precision_score(y_test, y_pred, zero_division=0),\n",
        "        'recall': recall_score(y_test, y_pred, zero_division=0),\n",
        "        'f1': f1_score(y_test, y_pred, zero_division=0),\n",
        "        'mcc': matthews_corrcoef(y_test, y_pred)\n",
        "    }\n",
        "    \n",
        "    # Confusion matrix\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    plt.figure(figsize=(6, 5))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
        "                xticklabels=['Normal', 'Attack'], \n",
        "                yticklabels=['Normal', 'Attack'])\n",
        "    plt.title(f'Confusion Matrix - {name}')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.savefig(f'{RESULTS_DIR}/{name}_cm.png')\n",
        "    plt.close()\n",
        "    \n",
        "    print(f\"\\n{name} Results:\")\n",
        "    for metric, value in metrics.items():\n",
        "        print(f\"  {metric}: {value:.4f}\")\n",
        "    \n",
        "    return metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Cell 5 — Run experiments\n",
        "models = {\n",
        "    'CNN': build_cnn((timesteps, 1)),\n",
        "    'LSTM': build_lstm((timesteps, 1)),\n",
        "    'CNN_LSTM': build_cnn_lstm((timesteps, 1))\n",
        "}\n",
        "\n",
        "print(\"Starting model training...\")\n",
        "results = []\n",
        "\n",
        "for name, model in models.items():\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"Training {name}\")\n",
        "    print(f\"{'='*50}\")\n",
        "    \n",
        "    trained_model, history = train_model(model, name)\n",
        "    metrics = evaluate_model(trained_model, name)\n",
        "    metrics['model'] = name\n",
        "    results.append(metrics)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Cell 6 — Display and save results\n",
        "print(f\"\\n{'='*50}\")\n",
        "print(\"FINAL RESULTS SUMMARY\")\n",
        "print(f\"{'='*50}\")\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "print(\"\\n\", results_df)\n",
        "\n",
        "# Save results\n",
        "results_df.to_csv(f'{RESULTS_DIR}/nsl_kdd_results.csv', index=False)\n",
        "print(f\"\\nResults saved to: {RESULTS_DIR}/nsl_kdd_results.csv\")\n",
        "\n",
        "# Plot comparison\n",
        "plt.figure(figsize=(10, 6))\n",
        "metrics_to_plot = ['accuracy', 'precision', 'recall', 'f1']\n",
        "x = range(len(results_df))\n",
        "\n",
        "for i, metric in enumerate(metrics_to_plot):\n",
        "    plt.bar([p + i*0.2 for p in x], results_df[metric], width=0.2, label=metric)\n",
        "\n",
        "plt.xlabel('Models')\n",
        "plt.ylabel('Score')\n",
        "plt.title('Model Performance Comparison')\n",
        "plt.xticks([p + 0.3 for p in x], results_df['model'])\n",
        "plt.legend()\n",
        "plt.ylim(0, 1)\n",
        "plt.tight_layout()\n",
        "plt.savefig(f'{RESULTS_DIR}/model_comparison.png')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
